{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDOS Path Analysis\n",
    "\n",
    "This notebook provides interactive analysis of end-to-end callback chains and timing paths.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Configuration\n",
    "2. Load Trace Data\n",
    "3. Callback Chain Analysis\n",
    "4. Path Timeline Visualization\n",
    "5. Latency Breakdown\n",
    "6. Bottleneck Identification\n",
    "7. Cross-Scenario Comparison\n",
    "8. Export Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "import yaml\n",
    "import warnings\n",
    "\n",
    "# Configure\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project paths\n",
    "WS_ROOT = Path('..').resolve()\n",
    "TRACES_DIR = WS_ROOT / 'traces'\n",
    "RESULTS_DIR = WS_ROOT / 'results'\n",
    "CONFIGS_DIR = WS_ROOT / 'configs'\n",
    "ANALYSIS_DIR = WS_ROOT / 'analysis'\n",
    "OUTPUT_DIR = ANALYSIS_DIR / 'output' / 'paths'\n",
    "\n",
    "print(f\"Workspace: {WS_ROOT}\")\n",
    "print(f\"Traces directory: {TRACES_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add analysis directory to path for imports\n",
    "sys.path.insert(0, str(ANALYSIS_DIR))\n",
    "\n",
    "# Try importing e2e_path_analyzer\n",
    "try:\n",
    "    from e2e_path_analyzer import E2EPathAnalyzer, CallbackInfo, CallbackChain\n",
    "    HAS_ANALYZER = True\n",
    "    print(\"E2E Path Analyzer loaded successfully\")\n",
    "except ImportError as e:\n",
    "    HAS_ANALYZER = False\n",
    "    print(f\"E2E Path Analyzer not available: {e}\")\n",
    "    print(\"Some features will be limited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load objectives configuration\n",
    "objectives_path = CONFIGS_DIR / 'objectives.yaml'\n",
    "\n",
    "if objectives_path.exists():\n",
    "    with open(objectives_path) as f:\n",
    "        objectives = yaml.safe_load(f)\n",
    "    print(\"Loaded objectives.yaml\")\n",
    "    print(f\"Defined paths: {list(objectives.get('paths', {}).keys())}\")\n",
    "else:\n",
    "    objectives = {}\n",
    "    print(\"objectives.yaml not found. Using defaults.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Trace Data\n",
    "\n",
    "Load callback timing data from experiment results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_trace_directories(base_dir: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Find all trace directories organized by scenario.\n",
    "    \"\"\"\n",
    "    trace_dirs = {}\n",
    "    \n",
    "    if not base_dir.exists():\n",
    "        return trace_dirs\n",
    "    \n",
    "    # Check both traces/ and results/ directories\n",
    "    for search_dir in [base_dir, base_dir.parent / 'results']:\n",
    "        if not search_dir.exists():\n",
    "            continue\n",
    "            \n",
    "        for scenario_dir in search_dir.iterdir():\n",
    "            if not scenario_dir.is_dir():\n",
    "                continue\n",
    "            \n",
    "            # Look for LTTng trace directories\n",
    "            lttng_dirs = list(scenario_dir.glob('**/metadata'))\n",
    "            if lttng_dirs:\n",
    "                trace_dirs[scenario_dir.name] = [d.parent for d in lttng_dirs]\n",
    "            \n",
    "            # Also check for processed callback data\n",
    "            callback_files = list(scenario_dir.glob('*callback*.json')) + \\\n",
    "                            list(scenario_dir.glob('*callback*.csv'))\n",
    "            if callback_files:\n",
    "                if scenario_dir.name not in trace_dirs:\n",
    "                    trace_dirs[scenario_dir.name] = []\n",
    "                trace_dirs[scenario_dir.name].extend(callback_files)\n",
    "    \n",
    "    return trace_dirs\n",
    "\n",
    "trace_dirs = find_trace_directories(TRACES_DIR)\n",
    "print(f\"Found traces for scenarios: {list(trace_dirs.keys())}\")\n",
    "\n",
    "for scenario, dirs in trace_dirs.items():\n",
    "    print(f\"  {scenario}: {len(dirs)} trace(s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_callback_data_from_results(results_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load callback timing data from result JSON files.\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    \n",
    "    if not results_dir.exists():\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for scenario_dir in results_dir.iterdir():\n",
    "        if not scenario_dir.is_dir():\n",
    "            continue\n",
    "        \n",
    "        scenario = scenario_dir.name\n",
    "        \n",
    "        for json_file in scenario_dir.glob('*_result.json'):\n",
    "            try:\n",
    "                with open(json_file) as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                # Extract callback timing if available\n",
    "                if 'callbacks' in data:\n",
    "                    for cb in data['callbacks']:\n",
    "                        cb['scenario'] = scenario\n",
    "                        cb['trial'] = json_file.stem\n",
    "                        all_rows.append(cb)\n",
    "                elif 'timing' in data:\n",
    "                    # Alternative format with timing breakdown\n",
    "                    for phase, timing in data['timing'].items():\n",
    "                        all_rows.append({\n",
    "                            'scenario': scenario,\n",
    "                            'trial': json_file.stem,\n",
    "                            'callback': phase,\n",
    "                            'duration_ms': timing.get('duration_ms', 0),\n",
    "                            'start_ns': timing.get('start_ns', 0),\n",
    "                            'end_ns': timing.get('end_ns', 0)\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    \n",
    "    return pd.DataFrame(all_rows) if all_rows else pd.DataFrame()\n",
    "\n",
    "# Try loading callback data\n",
    "callback_df = load_callback_data_from_results(RESULTS_DIR)\n",
    "print(f\"Loaded {len(callback_df)} callback records\")\n",
    "\n",
    "if len(callback_df) > 0:\n",
    "    display(callback_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic callback data for demonstration if no real data\n",
    "def create_demo_callback_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create synthetic callback chain data for demonstration.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Define typical ROS 2 callback chains for manipulation\n",
    "    chains = {\n",
    "        'planning': [\n",
    "            ('goal_callback', 5, 15),\n",
    "            ('compute_ik', 50, 150),\n",
    "            ('plan_path', 200, 800),\n",
    "            ('validate_trajectory', 20, 60),\n",
    "        ],\n",
    "        'control': [\n",
    "            ('joint_state_callback', 0.5, 1.5),\n",
    "            ('compute_command', 0.8, 2.0),\n",
    "            ('publish_command', 0.2, 0.5),\n",
    "        ],\n",
    "        'perception': [\n",
    "            ('image_callback', 10, 30),\n",
    "            ('process_pointcloud', 30, 80),\n",
    "            ('update_scene', 15, 40),\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    rows = []\n",
    "    scenarios = ['baseline', 'cpu_load', 'msg_load']\n",
    "    load_factors = {'baseline': 1.0, 'cpu_load': 1.8, 'msg_load': 1.3}\n",
    "    \n",
    "    for scenario in scenarios:\n",
    "        factor = load_factors[scenario]\n",
    "        \n",
    "        for trial_id in range(30):\n",
    "            for chain_name, callbacks in chains.items():\n",
    "                current_time = 0\n",
    "                \n",
    "                for cb_name, min_ms, max_ms in callbacks:\n",
    "                    # Add some noise and load factor\n",
    "                    duration = np.random.uniform(min_ms, max_ms) * factor\n",
    "                    duration *= np.random.uniform(0.9, 1.1)  # Additional noise\n",
    "                    \n",
    "                    rows.append({\n",
    "                        'scenario': scenario,\n",
    "                        'trial': f'trial_{trial_id:03d}',\n",
    "                        'chain': chain_name,\n",
    "                        'callback': cb_name,\n",
    "                        'start_ms': current_time,\n",
    "                        'duration_ms': duration,\n",
    "                        'end_ms': current_time + duration\n",
    "                    })\n",
    "                    \n",
    "                    current_time += duration\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Use demo data if no real data available\n",
    "if len(callback_df) == 0:\n",
    "    print(\"\\nNo callback data found. Using demo data for visualization.\")\n",
    "    callback_df = create_demo_callback_data()\n",
    "    print(f\"Created {len(callback_df)} demo callback records\")\n",
    "    \n",
    "    display(callback_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Callback Chain Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze callback chains\n",
    "if len(callback_df) > 0 and 'chain' in callback_df.columns:\n",
    "    print(\"=== Callback Chain Summary ===\")\n",
    "    \n",
    "    chain_summary = callback_df.groupby(['scenario', 'chain']).agg({\n",
    "        'duration_ms': ['mean', 'std', 'min', 'max', lambda x: x.quantile(0.95)],\n",
    "        'trial': 'nunique'\n",
    "    }).round(2)\n",
    "    \n",
    "    chain_summary.columns = ['mean_ms', 'std_ms', 'min_ms', 'max_ms', 'p95_ms', 'n_trials']\n",
    "    display(chain_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-callback analysis\n",
    "if len(callback_df) > 0:\n",
    "    print(\"\\n=== Per-Callback Statistics ===\")\n",
    "    \n",
    "    cb_summary = callback_df.groupby(['scenario', 'callback']).agg({\n",
    "        'duration_ms': ['mean', 'std', lambda x: x.quantile(0.95)]\n",
    "    }).round(2)\n",
    "    \n",
    "    cb_summary.columns = ['mean_ms', 'std_ms', 'p95_ms']\n",
    "    \n",
    "    # Reshape for comparison\n",
    "    cb_pivot = cb_summary.reset_index().pivot(\n",
    "        index='callback', \n",
    "        columns='scenario', \n",
    "        values='mean_ms'\n",
    "    )\n",
    "    \n",
    "    display(cb_pivot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Path Timeline Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_callback_timeline(df: pd.DataFrame, scenario: str, trial: str = None, chain: str = None):\n",
    "    \"\"\"\n",
    "    Plot a Gantt-style timeline of callback executions.\n",
    "    \"\"\"\n",
    "    # Filter data\n",
    "    plot_df = df[df['scenario'] == scenario].copy()\n",
    "    \n",
    "    if trial:\n",
    "        plot_df = plot_df[plot_df['trial'] == trial]\n",
    "    else:\n",
    "        # Use first trial\n",
    "        trial = plot_df['trial'].iloc[0]\n",
    "        plot_df = plot_df[plot_df['trial'] == trial]\n",
    "    \n",
    "    if chain:\n",
    "        plot_df = plot_df[plot_df['chain'] == chain]\n",
    "    \n",
    "    if len(plot_df) == 0:\n",
    "        print(f\"No data for scenario={scenario}, trial={trial}, chain={chain}\")\n",
    "        return\n",
    "    \n",
    "    # Create figure\n",
    "    fig, ax = plt.subplots(figsize=(14, max(6, len(plot_df) * 0.4)))\n",
    "    \n",
    "    # Color by chain\n",
    "    if 'chain' in plot_df.columns:\n",
    "        chains = plot_df['chain'].unique()\n",
    "        colors = dict(zip(chains, sns.color_palette('husl', len(chains))))\n",
    "    else:\n",
    "        colors = {'default': 'steelblue'}\n",
    "    \n",
    "    # Plot bars\n",
    "    y_positions = range(len(plot_df))\n",
    "    \n",
    "    for idx, (_, row) in enumerate(plot_df.iterrows()):\n",
    "        chain_name = row.get('chain', 'default')\n",
    "        color = colors.get(chain_name, 'gray')\n",
    "        \n",
    "        ax.barh(idx, row['duration_ms'], left=row.get('start_ms', 0),\n",
    "                height=0.6, color=color, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Add duration label\n",
    "        ax.text(row.get('start_ms', 0) + row['duration_ms'] / 2, idx,\n",
    "                f\"{row['duration_ms']:.1f}ms\", ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_yticks(y_positions)\n",
    "    ax.set_yticklabels(plot_df['callback'])\n",
    "    ax.set_xlabel('Time (ms)')\n",
    "    ax.set_ylabel('Callback')\n",
    "    ax.set_title(f'Callback Timeline - {scenario} ({trial})')\n",
    "    \n",
    "    # Legend\n",
    "    if 'chain' in plot_df.columns:\n",
    "        patches = [mpatches.Patch(color=c, label=name) for name, c in colors.items()]\n",
    "        ax.legend(handles=patches, loc='upper right')\n",
    "    \n",
    "    ax.invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(callback_df) > 0:\n",
    "    scenarios = callback_df['scenario'].unique()\n",
    "    print(f\"Available scenarios: {scenarios}\")\n",
    "    \n",
    "    # Plot timeline for first scenario\n",
    "    plot_callback_timeline(callback_df, scenarios[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare timelines across scenarios\n",
    "if len(callback_df) > 0 and 'chain' in callback_df.columns:\n",
    "    fig, axes = plt.subplots(1, len(callback_df['scenario'].unique()), \n",
    "                             figsize=(6*len(callback_df['scenario'].unique()), 8),\n",
    "                             sharey=True)\n",
    "    \n",
    "    if len(callback_df['scenario'].unique()) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Get average timings per callback\n",
    "    avg_timings = callback_df.groupby(['scenario', 'chain', 'callback']).agg({\n",
    "        'duration_ms': 'mean',\n",
    "        'start_ms': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    for ax, scenario in zip(axes, callback_df['scenario'].unique()):\n",
    "        scenario_df = avg_timings[avg_timings['scenario'] == scenario]\n",
    "        \n",
    "        chains = scenario_df['chain'].unique()\n",
    "        colors = dict(zip(chains, sns.color_palette('husl', len(chains))))\n",
    "        \n",
    "        y_positions = range(len(scenario_df))\n",
    "        \n",
    "        for idx, (_, row) in enumerate(scenario_df.iterrows()):\n",
    "            color = colors.get(row['chain'], 'gray')\n",
    "            ax.barh(idx, row['duration_ms'], left=row['start_ms'],\n",
    "                    height=0.6, color=color, alpha=0.8, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels(scenario_df['callback'])\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "        ax.set_title(f'{scenario}')\n",
    "        ax.invert_yaxis()\n",
    "    \n",
    "    plt.suptitle('Average Callback Timelines by Scenario', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Latency Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute latency breakdown per chain\n",
    "if len(callback_df) > 0 and 'chain' in callback_df.columns:\n",
    "    print(\"=== Latency Breakdown by Chain ===\")\n",
    "    \n",
    "    # Sum callback durations per trial and chain\n",
    "    chain_totals = callback_df.groupby(['scenario', 'trial', 'chain'])['duration_ms'].sum().reset_index()\n",
    "    \n",
    "    # Summary statistics\n",
    "    chain_stats = chain_totals.groupby(['scenario', 'chain'])['duration_ms'].agg([\n",
    "        'mean', 'std', 'min', 'max'\n",
    "    ]).round(2)\n",
    "    \n",
    "    display(chain_stats)\n",
    "    \n",
    "    # Stacked bar chart\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    scenarios = callback_df['scenario'].unique()\n",
    "    chains = callback_df['chain'].unique()\n",
    "    \n",
    "    # Compute mean per scenario/chain\n",
    "    pivot = chain_totals.groupby(['scenario', 'chain'])['duration_ms'].mean().unstack(fill_value=0)\n",
    "    \n",
    "    pivot.plot(kind='bar', stacked=True, ax=ax, colormap='husl')\n",
    "    \n",
    "    ax.set_xlabel('Scenario')\n",
    "    ax.set_ylabel('Total Latency (ms)')\n",
    "    ax.set_title('Latency Breakdown by Callback Chain')\n",
    "    ax.legend(title='Chain', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage breakdown\n",
    "if len(callback_df) > 0 and 'chain' in callback_df.columns:\n",
    "    print(\"\\n=== Percentage Breakdown ===\")\n",
    "    \n",
    "    # Mean callback durations\n",
    "    mean_durations = callback_df.groupby(['scenario', 'callback'])['duration_ms'].mean()\n",
    "    \n",
    "    for scenario in callback_df['scenario'].unique():\n",
    "        scenario_total = mean_durations[scenario].sum()\n",
    "        print(f\"\\n{scenario} (total: {scenario_total:.1f}ms):\")\n",
    "        \n",
    "        for callback, duration in mean_durations[scenario].sort_values(ascending=False).items():\n",
    "            pct = duration / scenario_total * 100\n",
    "            print(f\"  {callback}: {duration:.1f}ms ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bottleneck Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_bottlenecks(df: pd.DataFrame, threshold_pct: float = 20.0):\n",
    "    \"\"\"\n",
    "    Identify callbacks that consume more than threshold_pct of total latency.\n",
    "    \"\"\"\n",
    "    bottlenecks = []\n",
    "    \n",
    "    for scenario in df['scenario'].unique():\n",
    "        scenario_df = df[df['scenario'] == scenario]\n",
    "        \n",
    "        # Mean duration per callback\n",
    "        mean_durations = scenario_df.groupby('callback')['duration_ms'].mean()\n",
    "        total = mean_durations.sum()\n",
    "        \n",
    "        for callback, duration in mean_durations.items():\n",
    "            pct = duration / total * 100\n",
    "            if pct >= threshold_pct:\n",
    "                bottlenecks.append({\n",
    "                    'scenario': scenario,\n",
    "                    'callback': callback,\n",
    "                    'mean_ms': duration,\n",
    "                    'pct_total': pct,\n",
    "                    'is_bottleneck': True\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(bottlenecks)\n",
    "\n",
    "if len(callback_df) > 0:\n",
    "    print(\"=== Bottleneck Analysis (>20% of total latency) ===\")\n",
    "    \n",
    "    bottleneck_df = identify_bottlenecks(callback_df, threshold_pct=20.0)\n",
    "    \n",
    "    if len(bottleneck_df) > 0:\n",
    "        display(bottleneck_df.sort_values('pct_total', ascending=False))\n",
    "    else:\n",
    "        print(\"No callbacks exceed 20% of total latency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of callback contributions\n",
    "if len(callback_df) > 0:\n",
    "    # Compute percentage contribution\n",
    "    pct_contribution = callback_df.groupby(['scenario', 'callback'])['duration_ms'].mean().unstack(fill_value=0)\n",
    "    pct_contribution = pct_contribution.div(pct_contribution.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    sns.heatmap(pct_contribution, annot=True, fmt='.1f', cmap='YlOrRd',\n",
    "                cbar_kws={'label': '% of Total Latency'}, ax=ax)\n",
    "    \n",
    "    ax.set_title('Callback Contribution to Total Latency (%)')\n",
    "    ax.set_xlabel('Callback')\n",
    "    ax.set_ylabel('Scenario')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify load-sensitive callbacks\n",
    "if len(callback_df) > 0 and 'baseline' in callback_df['scenario'].unique():\n",
    "    print(\"\\n=== Load-Sensitive Callbacks ===\")\n",
    "    print(\"(Callbacks with largest increase under load)\\n\")\n",
    "    \n",
    "    # Mean durations\n",
    "    mean_by_scenario = callback_df.groupby(['scenario', 'callback'])['duration_ms'].mean().unstack(level=0)\n",
    "    \n",
    "    if 'baseline' in mean_by_scenario.columns:\n",
    "        for load_scenario in [s for s in mean_by_scenario.columns if s != 'baseline']:\n",
    "            pct_change = ((mean_by_scenario[load_scenario] - mean_by_scenario['baseline']) / \n",
    "                         mean_by_scenario['baseline'] * 100)\n",
    "            \n",
    "            print(f\"\\n{load_scenario} vs baseline:\")\n",
    "            for callback, change in pct_change.sort_values(ascending=False).head(5).items():\n",
    "                baseline_val = mean_by_scenario.loc[callback, 'baseline']\n",
    "                load_val = mean_by_scenario.loc[callback, load_scenario]\n",
    "                print(f\"  {callback}: {baseline_val:.1f}ms -> {load_val:.1f}ms ({change:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Scenario Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots per callback across scenarios\n",
    "if len(callback_df) > 0:\n",
    "    # Select top callbacks by mean duration\n",
    "    top_callbacks = callback_df.groupby('callback')['duration_ms'].mean().nlargest(6).index.tolist()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for ax, callback in zip(axes, top_callbacks):\n",
    "        data = callback_df[callback_df['callback'] == callback]\n",
    "        \n",
    "        sns.boxplot(data=data, x='scenario', y='duration_ms', ax=ax)\n",
    "        ax.set_title(f'{callback}')\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('Duration (ms)')\n",
    "    \n",
    "    plt.suptitle('Callback Duration Distribution Across Scenarios', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain-level comparison\n",
    "if len(callback_df) > 0 and 'chain' in callback_df.columns:\n",
    "    # Total chain duration per trial\n",
    "    chain_durations = callback_df.groupby(['scenario', 'trial', 'chain'])['duration_ms'].sum().reset_index()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    sns.boxplot(data=chain_durations, x='chain', y='duration_ms', hue='scenario', ax=ax)\n",
    "    \n",
    "    ax.set_xlabel('Callback Chain')\n",
    "    ax.set_ylabel('Total Duration (ms)')\n",
    "    ax.set_title('Chain Duration Distribution by Scenario')\n",
    "    ax.legend(title='Scenario')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_path_analysis(callback_df: pd.DataFrame, bottleneck_df: pd.DataFrame, \n",
    "                         output_dir: Path):\n",
    "    \"\"\"\n",
    "    Export path analysis results.\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Callback statistics\n",
    "    if len(callback_df) > 0:\n",
    "        cb_stats = callback_df.groupby(['scenario', 'callback']).agg({\n",
    "            'duration_ms': ['mean', 'std', 'min', 'max', lambda x: x.quantile(0.95)]\n",
    "        }).round(2)\n",
    "        cb_stats.columns = ['mean_ms', 'std_ms', 'min_ms', 'max_ms', 'p95_ms']\n",
    "        cb_stats.to_csv(output_dir / 'callback_statistics.csv')\n",
    "        print(f\"Saved: {output_dir / 'callback_statistics.csv'}\")\n",
    "    \n",
    "    # Bottlenecks\n",
    "    if len(bottleneck_df) > 0:\n",
    "        bottleneck_df.to_csv(output_dir / 'bottlenecks.csv', index=False)\n",
    "        print(f\"Saved: {output_dir / 'bottlenecks.csv'}\")\n",
    "    \n",
    "    # Generate Mermaid diagram\n",
    "    mermaid = generate_mermaid_diagram(callback_df)\n",
    "    with open(output_dir / 'callback_flow.mmd', 'w') as f:\n",
    "        f.write(mermaid)\n",
    "    print(f\"Saved: {output_dir / 'callback_flow.mmd'}\")\n",
    "\n",
    "def generate_mermaid_diagram(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Generate a Mermaid flowchart of callback chains.\n",
    "    \"\"\"\n",
    "    lines = ['flowchart TD']\n",
    "    \n",
    "    if 'chain' not in df.columns:\n",
    "        return ''\n",
    "    \n",
    "    # Get unique chains and callbacks\n",
    "    for chain in df['chain'].unique():\n",
    "        chain_df = df[df['chain'] == chain].drop_duplicates('callback')\n",
    "        callbacks = chain_df['callback'].tolist()\n",
    "        \n",
    "        # Add subgraph\n",
    "        lines.append(f'    subgraph {chain}')\n",
    "        \n",
    "        for i, cb in enumerate(callbacks):\n",
    "            cb_id = cb.replace('_', '')\n",
    "            mean_ms = df[df['callback'] == cb]['duration_ms'].mean()\n",
    "            lines.append(f'        {cb_id}[\"{cb}<br/>{mean_ms:.1f}ms\"]')\n",
    "            \n",
    "            if i > 0:\n",
    "                prev_id = callbacks[i-1].replace('_', '')\n",
    "                lines.append(f'        {prev_id} --> {cb_id}')\n",
    "        \n",
    "        lines.append('    end')\n",
    "    \n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# Uncomment to export\n",
    "# if len(callback_df) > 0:\n",
    "#     bottleneck_df = identify_bottlenecks(callback_df) if 'bottleneck_df' not in dir() else bottleneck_df\n",
    "#     export_path_analysis(callback_df, bottleneck_df, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary markdown\n",
    "if len(callback_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PATH ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nTotal callback records: {len(callback_df)}\")\n",
    "    print(f\"Scenarios: {callback_df['scenario'].nunique()}\")\n",
    "    print(f\"Unique callbacks: {callback_df['callback'].nunique()}\")\n",
    "    \n",
    "    if 'chain' in callback_df.columns:\n",
    "        print(f\"Callback chains: {callback_df['chain'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nTop 5 Slowest Callbacks (mean):\")\n",
    "    top_slow = callback_df.groupby('callback')['duration_ms'].mean().nlargest(5)\n",
    "    for cb, duration in top_slow.items():\n",
    "        print(f\"  {cb}: {duration:.1f}ms\")\n",
    "    \n",
    "    if 'bottleneck_df' in dir() and len(bottleneck_df) > 0:\n",
    "        print(f\"\\nBottlenecks identified: {len(bottleneck_df)}\")\n",
    "        for _, row in bottleneck_df.iterrows():\n",
    "            print(f\"  {row['scenario']}/{row['callback']}: {row['pct_total']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run experiments with tracing enabled:\n",
    "   ```bash\n",
    "   cd ~/ldos_manip_tracing\n",
    "   make run_all NUM_TRIALS=30\n",
    "   ```\n",
    "\n",
    "2. Analyze traces with e2e_path_analyzer:\n",
    "   ```bash\n",
    "   python3 analysis/e2e_path_analyzer.py --trace-dir traces/baseline/ --output analysis/output/paths/\n",
    "   ```\n",
    "\n",
    "3. Review generated reports and Mermaid diagrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
