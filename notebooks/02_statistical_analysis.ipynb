{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDOS Statistical Analysis\n",
    "\n",
    "This notebook performs rigorous statistical analysis of experiment results.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Data Loading\n",
    "2. Normality Testing\n",
    "3. Two-Sample Comparisons (Baseline vs Load)\n",
    "4. Multi-Group ANOVA\n",
    "5. Effect Size Analysis\n",
    "6. Confidence Intervals\n",
    "7. Power Analysis\n",
    "8. Export Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Statistical imports\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, levene, ttest_ind, mannwhitneyu, f_oneway, kruskal\n",
    "\n",
    "try:\n",
    "    from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "    from statsmodels.stats.power import TTestIndPower\n",
    "    HAS_STATSMODELS = True\n",
    "except ImportError:\n",
    "    HAS_STATSMODELS = False\n",
    "    print(\"Warning: statsmodels not installed. Some tests unavailable.\")\n",
    "    print(\"Install with: pip install statsmodels\")\n",
    "\n",
    "# Add analysis directory to path\n",
    "sys.path.insert(0, str(Path('..').resolve() / 'analysis'))\n",
    "\n",
    "try:\n",
    "    from stats_utils import (\n",
    "        confidence_interval, cohens_d, compare_scenarios, \n",
    "        anova_with_posthoc, required_sample_size\n",
    "    )\n",
    "    HAS_STATS_UTILS = True\n",
    "except ImportError:\n",
    "    HAS_STATS_UTILS = False\n",
    "    print(\"Note: stats_utils.py not found. Using inline implementations.\")\n",
    "\n",
    "# Configure\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project paths\n",
    "WS_ROOT = Path('..').resolve()\n",
    "RESULTS_DIR = WS_ROOT / 'results'\n",
    "ANALYSIS_OUTPUT = WS_ROOT / 'analysis' / 'output'\n",
    "\n",
    "print(f\"scipy version: {stats.scipy_version}\" if hasattr(stats, 'scipy_version') else f\"scipy loaded\")\n",
    "print(f\"statsmodels available: {HAS_STATSMODELS}\")\n",
    "print(f\"stats_utils available: {HAS_STATS_UTILS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inline implementations if stats_utils not available\n",
    "if not HAS_STATS_UTILS:\n",
    "    def confidence_interval(data, confidence=0.95):\n",
    "        \"\"\"Compute confidence interval for the mean.\"\"\"\n",
    "        data = np.array(data)\n",
    "        n = len(data)\n",
    "        mean = np.mean(data)\n",
    "        se = stats.sem(data)\n",
    "        h = se * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "        return mean, mean - h, mean + h\n",
    "    \n",
    "    def cohens_d(group1, group2):\n",
    "        \"\"\"Compute Cohen's d effect size.\"\"\"\n",
    "        n1, n2 = len(group1), len(group2)\n",
    "        var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "        pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "        return (np.mean(group1) - np.mean(group2)) / pooled_std if pooled_std > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data (same as exploration notebook)\n",
    "def load_combined_results(results_dir: Path) -> pd.DataFrame:\n",
    "    all_rows = []\n",
    "    \n",
    "    if not results_dir.exists():\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for scenario_dir in results_dir.iterdir():\n",
    "        if not scenario_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        scenario_name = scenario_dir.name\n",
    "        json_files = list(scenario_dir.glob('*_result.json'))\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                with open(json_file) as f:\n",
    "                    data = json.load(f)\n",
    "                data['scenario'] = scenario_name\n",
    "                all_rows.append(data)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Fallback to summary.csv\n",
    "        if not json_files:\n",
    "            summary_csv = scenario_dir / 'summary.csv'\n",
    "            if summary_csv.exists():\n",
    "                try:\n",
    "                    df_summary = pd.read_csv(summary_csv)\n",
    "                    df_summary['scenario'] = scenario_name\n",
    "                    all_rows.extend(df_summary.to_dict('records'))\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    return pd.DataFrame(all_rows) if all_rows else pd.DataFrame()\n",
    "\n",
    "df = load_combined_results(RESULTS_DIR)\n",
    "print(f\"Loaded {len(df)} trials from {df['scenario'].nunique() if len(df) > 0 else 0} scenarios\")\n",
    "\n",
    "# Filter to successful trials\n",
    "if len(df) > 0 and 'status' in df.columns:\n",
    "    df_success = df[df['status'] == 'success'].copy()\n",
    "    print(f\"Successful trials: {len(df_success)}\")\n",
    "else:\n",
    "    df_success = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Normality Testing\n",
    "\n",
    "Before parametric tests, check if data is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_normality(data, name=\"Data\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    Test for normality using Shapiro-Wilk test.\n",
    "    \n",
    "    Returns:\n",
    "        dict with statistic, p_value, and is_normal flag\n",
    "    \"\"\"\n",
    "    data = np.array(data).flatten()\n",
    "    data = data[~np.isnan(data)]\n",
    "    \n",
    "    if len(data) < 3:\n",
    "        return {'name': name, 'statistic': np.nan, 'p_value': np.nan, 'is_normal': False, 'n': len(data)}\n",
    "    \n",
    "    # Shapiro-Wilk (up to 5000 samples)\n",
    "    if len(data) > 5000:\n",
    "        data = np.random.choice(data, 5000, replace=False)\n",
    "    \n",
    "    stat, p_value = shapiro(data)\n",
    "    is_normal = p_value > alpha\n",
    "    \n",
    "    return {\n",
    "        'name': name,\n",
    "        'n': len(data),\n",
    "        'statistic': stat,\n",
    "        'p_value': p_value,\n",
    "        'is_normal': is_normal\n",
    "    }\n",
    "\n",
    "if len(df_success) > 0:\n",
    "    print(\"=== Normality Tests (Shapiro-Wilk) ===\")\n",
    "    print(\"H0: Data is normally distributed\")\n",
    "    print(\"p < 0.05 suggests non-normality\\n\")\n",
    "    \n",
    "    normality_results = []\n",
    "    \n",
    "    for col in ['total_latency_ms', 'planning_latency_ms', 'execution_latency_ms']:\n",
    "        if col not in df_success.columns:\n",
    "            continue\n",
    "            \n",
    "        for scenario in df_success['scenario'].unique():\n",
    "            data = df_success[df_success['scenario'] == scenario][col].dropna()\n",
    "            result = test_normality(data, f\"{scenario}/{col}\")\n",
    "            normality_results.append(result)\n",
    "    \n",
    "    normality_df = pd.DataFrame(normality_results)\n",
    "    display(normality_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Q plots for visual normality check\n",
    "if len(df_success) > 0 and 'total_latency_ms' in df_success.columns:\n",
    "    col = 'total_latency_ms'\n",
    "    scenarios = df_success['scenario'].unique()\n",
    "    n_scenarios = len(scenarios)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_scenarios, figsize=(5*n_scenarios, 4))\n",
    "    if n_scenarios == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, scenario in zip(axes, scenarios):\n",
    "        data = df_success[df_success['scenario'] == scenario][col].dropna()\n",
    "        stats.probplot(data, dist=\"norm\", plot=ax)\n",
    "        ax.set_title(f'{scenario}\\nQ-Q Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Two-Sample Comparisons (Baseline vs Load)\n",
    "\n",
    "Compare baseline to each load scenario using appropriate statistical tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_two_groups(group1, group2, name1=\"Group1\", name2=\"Group2\", alpha=0.05):\n",
    "    \"\"\"\n",
    "    Comprehensive comparison of two groups.\n",
    "    \n",
    "    Performs:\n",
    "    - Levene's test for equal variances\n",
    "    - Two-sample t-test (Welch's if variances unequal)\n",
    "    - Mann-Whitney U test (non-parametric alternative)\n",
    "    - Cohen's d effect size\n",
    "    \"\"\"\n",
    "    g1 = np.array(group1).flatten()\n",
    "    g2 = np.array(group2).flatten()\n",
    "    g1 = g1[~np.isnan(g1)]\n",
    "    g2 = g2[~np.isnan(g2)]\n",
    "    \n",
    "    results = {\n",
    "        'comparison': f\"{name1} vs {name2}\",\n",
    "        'n1': len(g1),\n",
    "        'n2': len(g2),\n",
    "        'mean1': np.mean(g1),\n",
    "        'mean2': np.mean(g2),\n",
    "        'std1': np.std(g1, ddof=1),\n",
    "        'std2': np.std(g2, ddof=1),\n",
    "    }\n",
    "    \n",
    "    # Percent change\n",
    "    if results['mean1'] > 0:\n",
    "        results['pct_change'] = (results['mean2'] - results['mean1']) / results['mean1'] * 100\n",
    "    else:\n",
    "        results['pct_change'] = np.nan\n",
    "    \n",
    "    # Levene's test for equal variances\n",
    "    levene_stat, levene_p = levene(g1, g2)\n",
    "    equal_var = levene_p > alpha\n",
    "    results['levene_p'] = levene_p\n",
    "    results['equal_variance'] = equal_var\n",
    "    \n",
    "    # T-test (Welch's if unequal variance)\n",
    "    t_stat, t_p = ttest_ind(g1, g2, equal_var=equal_var)\n",
    "    results['t_statistic'] = t_stat\n",
    "    results['t_p_value'] = t_p\n",
    "    results['t_significant'] = t_p < alpha\n",
    "    \n",
    "    # Mann-Whitney U (non-parametric)\n",
    "    u_stat, u_p = mannwhitneyu(g1, g2, alternative='two-sided')\n",
    "    results['mwu_statistic'] = u_stat\n",
    "    results['mwu_p_value'] = u_p\n",
    "    results['mwu_significant'] = u_p < alpha\n",
    "    \n",
    "    # Cohen's d\n",
    "    d = cohens_d(g1, g2)\n",
    "    results['cohens_d'] = d\n",
    "    \n",
    "    # Effect size interpretation\n",
    "    abs_d = abs(d)\n",
    "    if abs_d < 0.2:\n",
    "        results['effect_size'] = 'negligible'\n",
    "    elif abs_d < 0.5:\n",
    "        results['effect_size'] = 'small'\n",
    "    elif abs_d < 0.8:\n",
    "        results['effect_size'] = 'medium'\n",
    "    else:\n",
    "        results['effect_size'] = 'large'\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline to each load scenario\n",
    "if len(df_success) > 0 and 'total_latency_ms' in df_success.columns:\n",
    "    col = 'total_latency_ms'\n",
    "    scenarios = df_success['scenario'].unique()\n",
    "    \n",
    "    print(\"=== Two-Sample Comparisons ===\")\n",
    "    print(f\"Metric: {col}\\n\")\n",
    "    \n",
    "    comparison_results = []\n",
    "    \n",
    "    if 'baseline' in scenarios:\n",
    "        baseline_data = df_success[df_success['scenario'] == 'baseline'][col].dropna()\n",
    "        \n",
    "        for scenario in scenarios:\n",
    "            if scenario == 'baseline':\n",
    "                continue\n",
    "            \n",
    "            load_data = df_success[df_success['scenario'] == scenario][col].dropna()\n",
    "            result = compare_two_groups(baseline_data, load_data, 'baseline', scenario)\n",
    "            comparison_results.append(result)\n",
    "        \n",
    "        if comparison_results:\n",
    "            comparison_df = pd.DataFrame(comparison_results)\n",
    "            display(comparison_df[['comparison', 'n1', 'n2', 'mean1', 'mean2', \n",
    "                                   'pct_change', 't_p_value', 't_significant',\n",
    "                                   'cohens_d', 'effect_size']])\n",
    "    else:\n",
    "        print(\"No 'baseline' scenario found. Available:\", list(scenarios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison results\n",
    "if len(df_success) > 0 and 'comparison_results' in dir() and comparison_results:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    comp_df = pd.DataFrame(comparison_results)\n",
    "    \n",
    "    # Bar chart of means with error bars\n",
    "    ax1 = axes[0]\n",
    "    x = range(len(comp_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar([i - width/2 for i in x], comp_df['mean1'], width, \n",
    "                    yerr=comp_df['std1'], label='Baseline', capsize=3)\n",
    "    bars2 = ax1.bar([i + width/2 for i in x], comp_df['mean2'], width,\n",
    "                    yerr=comp_df['std2'], label='Load', capsize=3)\n",
    "    \n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels([r['comparison'].split(' vs ')[1] for r in comparison_results], rotation=45)\n",
    "    ax1.set_ylabel('Latency (ms)')\n",
    "    ax1.set_title('Mean Latency Comparison')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Add significance stars\n",
    "    for i, (bar1, bar2, result) in enumerate(zip(bars1, bars2, comparison_results)):\n",
    "        if result['t_significant']:\n",
    "            max_height = max(bar1.get_height() + result['std1'], bar2.get_height() + result['std2'])\n",
    "            ax1.text(i, max_height * 1.05, '*', ha='center', fontsize=16, color='red')\n",
    "    \n",
    "    # Effect size bar chart\n",
    "    ax2 = axes[1]\n",
    "    colors = ['green' if abs(d) < 0.2 else 'gold' if abs(d) < 0.5 else 'orange' if abs(d) < 0.8 else 'red'\n",
    "              for d in comp_df['cohens_d']]\n",
    "    bars = ax2.bar(x, comp_df['cohens_d'], color=colors)\n",
    "    ax2.axhline(y=0.2, color='green', linestyle='--', alpha=0.5, label='Small (0.2)')\n",
    "    ax2.axhline(y=0.5, color='orange', linestyle='--', alpha=0.5, label='Medium (0.5)')\n",
    "    ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.5, label='Large (0.8)')\n",
    "    ax2.axhline(y=-0.2, color='green', linestyle='--', alpha=0.5)\n",
    "    ax2.axhline(y=-0.5, color='orange', linestyle='--', alpha=0.5)\n",
    "    ax2.axhline(y=-0.8, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels([r['comparison'].split(' vs ')[1] for r in comparison_results], rotation=45)\n",
    "    ax2.set_ylabel(\"Cohen's d\")\n",
    "    ax2.set_title('Effect Sizes (Cohen\\'s d)')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Group ANOVA\n",
    "\n",
    "Compare all scenarios simultaneously using ANOVA with post-hoc tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_anova(df: pd.DataFrame, col: str, group_col: str = 'scenario', alpha: float = 0.05):\n",
    "    \"\"\"\n",
    "    Run one-way ANOVA with post-hoc Tukey HSD.\n",
    "    \"\"\"\n",
    "    groups = [df[df[group_col] == g][col].dropna().values for g in df[group_col].unique()]\n",
    "    group_names = df[group_col].unique().tolist()\n",
    "    \n",
    "    # Check if we have enough groups\n",
    "    if len(groups) < 2:\n",
    "        print(\"Need at least 2 groups for ANOVA\")\n",
    "        return None, None\n",
    "    \n",
    "    # One-way ANOVA\n",
    "    f_stat, p_value = f_oneway(*groups)\n",
    "    \n",
    "    print(\"=== One-Way ANOVA ===\")\n",
    "    print(f\"F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"p-value: {p_value:.6f}\")\n",
    "    print(f\"Significant (p < {alpha}): {p_value < alpha}\")\n",
    "    \n",
    "    # Kruskal-Wallis (non-parametric alternative)\n",
    "    h_stat, kw_p = kruskal(*groups)\n",
    "    print(f\"\\nKruskal-Wallis H-statistic: {h_stat:.4f}\")\n",
    "    print(f\"Kruskal-Wallis p-value: {kw_p:.6f}\")\n",
    "    \n",
    "    # Post-hoc Tukey HSD\n",
    "    tukey_results = None\n",
    "    if HAS_STATSMODELS and p_value < alpha:\n",
    "        print(\"\\n=== Post-Hoc Tukey HSD ===\")\n",
    "        # Prepare data for Tukey\n",
    "        tukey_data = df[[col, group_col]].dropna()\n",
    "        tukey_results = pairwise_tukeyhsd(tukey_data[col], tukey_data[group_col], alpha=alpha)\n",
    "        print(tukey_results)\n",
    "    \n",
    "    return {'f_stat': f_stat, 'p_value': p_value, 'h_stat': h_stat, 'kw_p': kw_p}, tukey_results\n",
    "\n",
    "if len(df_success) > 0 and 'total_latency_ms' in df_success.columns:\n",
    "    anova_result, tukey = run_anova(df_success, 'total_latency_ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Tukey results\n",
    "if HAS_STATSMODELS and 'tukey' in dir() and tukey is not None:\n",
    "    fig = tukey.plot_simultaneous(figsize=(10, 6))\n",
    "    plt.title('Tukey HSD - 95% Confidence Intervals')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Effect Size Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_effect_sizes(df: pd.DataFrame, col: str, reference: str = 'baseline'):\n",
    "    \"\"\"\n",
    "    Compute effect sizes for all scenarios vs reference.\n",
    "    \"\"\"\n",
    "    scenarios = df['scenario'].unique()\n",
    "    \n",
    "    if reference not in scenarios:\n",
    "        print(f\"Reference '{reference}' not found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    ref_data = df[df['scenario'] == reference][col].dropna().values\n",
    "    \n",
    "    results = []\n",
    "    for scenario in scenarios:\n",
    "        if scenario == reference:\n",
    "            continue\n",
    "        \n",
    "        scenario_data = df[df['scenario'] == scenario][col].dropna().values\n",
    "        d = cohens_d(ref_data, scenario_data)\n",
    "        \n",
    "        # Effect size interpretation\n",
    "        abs_d = abs(d)\n",
    "        if abs_d < 0.2:\n",
    "            interpretation = 'negligible'\n",
    "        elif abs_d < 0.5:\n",
    "            interpretation = 'small'\n",
    "        elif abs_d < 0.8:\n",
    "            interpretation = 'medium'\n",
    "        else:\n",
    "            interpretation = 'large'\n",
    "        \n",
    "        results.append({\n",
    "            'scenario': scenario,\n",
    "            'cohens_d': d,\n",
    "            'abs_d': abs_d,\n",
    "            'interpretation': interpretation,\n",
    "            'direction': 'increase' if d > 0 else 'decrease'\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if len(df_success) > 0 and 'total_latency_ms' in df_success.columns:\n",
    "    print(\"=== Effect Sizes (Cohen's d) ===\")\n",
    "    print(\"Reference: baseline\\n\")\n",
    "    \n",
    "    effect_df = compute_all_effect_sizes(df_success, 'total_latency_ms')\n",
    "    if len(effect_df) > 0:\n",
    "        display(effect_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence_intervals(df: pd.DataFrame, col: str, confidence: float = 0.95):\n",
    "    \"\"\"\n",
    "    Compute confidence intervals for each scenario.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for scenario in df['scenario'].unique():\n",
    "        data = df[df['scenario'] == scenario][col].dropna().values\n",
    "        \n",
    "        if len(data) < 2:\n",
    "            continue\n",
    "        \n",
    "        mean, lower, upper = confidence_interval(data, confidence)\n",
    "        \n",
    "        results.append({\n",
    "            'scenario': scenario,\n",
    "            'n': len(data),\n",
    "            'mean': mean,\n",
    "            'ci_lower': lower,\n",
    "            'ci_upper': upper,\n",
    "            'margin': upper - mean,\n",
    "            'std': np.std(data, ddof=1)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "if len(df_success) > 0 and 'total_latency_ms' in df_success.columns:\n",
    "    print(\"=== 95% Confidence Intervals ===\")\n",
    "    \n",
    "    ci_df = compute_confidence_intervals(df_success, 'total_latency_ms')\n",
    "    display(ci_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confidence intervals\n",
    "if len(df_success) > 0 and 'ci_df' in dir() and len(ci_df) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    x = range(len(ci_df))\n",
    "    \n",
    "    ax.errorbar(x, ci_df['mean'], \n",
    "                yerr=[ci_df['mean'] - ci_df['ci_lower'], ci_df['ci_upper'] - ci_df['mean']],\n",
    "                fmt='o', capsize=5, capthick=2, markersize=8)\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(ci_df['scenario'], rotation=45)\n",
    "    ax.set_ylabel('Latency (ms)')\n",
    "    ax.set_title('Mean Latency with 95% Confidence Intervals')\n",
    "    \n",
    "    # Add sample sizes\n",
    "    for i, row in ci_df.iterrows():\n",
    "        ax.annotate(f'n={row[\"n\"]}', (i, row['ci_upper']), \n",
    "                   textcoords='offset points', xytext=(0, 10), ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Power Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_analysis_sample_size(effect_size: float, power: float = 0.8, alpha: float = 0.05):\n",
    "    \"\"\"\n",
    "    Compute required sample size for desired power.\n",
    "    \"\"\"\n",
    "    if HAS_STATSMODELS:\n",
    "        analysis = TTestIndPower()\n",
    "        n = analysis.solve_power(effect_size=effect_size, power=power, alpha=alpha)\n",
    "        return int(np.ceil(n))\n",
    "    else:\n",
    "        # Approximation using z-scores\n",
    "        z_alpha = stats.norm.ppf(1 - alpha/2)\n",
    "        z_beta = stats.norm.ppf(power)\n",
    "        n = 2 * ((z_alpha + z_beta) / effect_size) ** 2\n",
    "        return int(np.ceil(n))\n",
    "\n",
    "print(\"=== Required Sample Sizes for 80% Power ===\")\n",
    "print(\"(Two-sample t-test, alpha=0.05)\\n\")\n",
    "\n",
    "effect_sizes = [0.2, 0.5, 0.8, 1.0, 1.2]\n",
    "for d in effect_sizes:\n",
    "    n = power_analysis_sample_size(d)\n",
    "    label = 'small' if d == 0.2 else 'medium' if d == 0.5 else 'large' if d == 0.8 else ''\n",
    "    print(f\"d = {d:.1f} ({label}): n = {n} per group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Achieved power for current sample sizes\n",
    "if HAS_STATSMODELS and len(df_success) > 0 and 'comparison_results' in dir() and comparison_results:\n",
    "    print(\"\\n=== Achieved Power for Current Experiments ===\")\n",
    "    \n",
    "    analysis = TTestIndPower()\n",
    "    \n",
    "    for result in comparison_results:\n",
    "        d = abs(result['cohens_d'])\n",
    "        n1, n2 = result['n1'], result['n2']\n",
    "        \n",
    "        # Use harmonic mean of sample sizes\n",
    "        n_eff = 2 * n1 * n2 / (n1 + n2)\n",
    "        \n",
    "        if d > 0:\n",
    "            power = analysis.solve_power(effect_size=d, nobs1=n_eff, alpha=0.05)\n",
    "            print(f\"{result['comparison']}: d={d:.2f}, n_eff={n_eff:.0f}, power={power:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_statistical_summary(comparison_df: pd.DataFrame, ci_df: pd.DataFrame, \n",
    "                               output_dir: Path):\n",
    "    \"\"\"\n",
    "    Export statistical results to various formats.\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # CSV exports\n",
    "    comparison_df.to_csv(output_dir / 'statistical_comparisons.csv', index=False)\n",
    "    ci_df.to_csv(output_dir / 'confidence_intervals.csv', index=False)\n",
    "    \n",
    "    print(f\"Saved: {output_dir / 'statistical_comparisons.csv'}\")\n",
    "    print(f\"Saved: {output_dir / 'confidence_intervals.csv'}\")\n",
    "    \n",
    "    # LaTeX table for comparison results\n",
    "    latex_cols = ['comparison', 'pct_change', 't_p_value', 'cohens_d', 'effect_size']\n",
    "    if all(c in comparison_df.columns for c in latex_cols):\n",
    "        latex_df = comparison_df[latex_cols].copy()\n",
    "        latex_df['pct_change'] = latex_df['pct_change'].apply(lambda x: f\"{x:+.1f}%\")\n",
    "        latex_df['t_p_value'] = latex_df['t_p_value'].apply(lambda x: f\"{x:.4f}\" if x >= 0.001 else \"<0.001\")\n",
    "        latex_df['cohens_d'] = latex_df['cohens_d'].apply(lambda x: f\"{x:.2f}\")\n",
    "        \n",
    "        latex = latex_df.to_latex(\n",
    "            index=False,\n",
    "            caption='Statistical Comparison of Scenarios',\n",
    "            label='tab:stat_comparison',\n",
    "            column_format='l' + 'r' * (len(latex_cols) - 1)\n",
    "        )\n",
    "        \n",
    "        with open(output_dir / 'stat_comparison.tex', 'w') as f:\n",
    "            f.write(latex)\n",
    "        print(f\"Saved: {output_dir / 'stat_comparison.tex'}\")\n",
    "\n",
    "# Uncomment to export\n",
    "# if 'comparison_results' in dir() and comparison_results and 'ci_df' in dir():\n",
    "#     export_statistical_summary(pd.DataFrame(comparison_results), ci_df, ANALYSIS_OUTPUT / 'stats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "if len(df_success) > 0 and 'comparison_results' in dir() and comparison_results:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"STATISTICAL ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nTotal successful trials: {len(df_success)}\")\n",
    "    print(f\"Scenarios analyzed: {df_success['scenario'].nunique()}\")\n",
    "    \n",
    "    print(\"\\nKey Findings:\")\n",
    "    for result in comparison_results:\n",
    "        sig = \"*\" if result['t_significant'] else \"\"\n",
    "        print(f\"  - {result['comparison']}: {result['pct_change']:+.1f}% change, \"\n",
    "              f\"d={result['cohens_d']:.2f} ({result['effect_size']}){sig}\")\n",
    "    \n",
    "    print(\"\\n* = statistically significant (p < 0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **`03_path_analysis.ipynb`** - Analyze end-to-end callback chains\n",
    "\n",
    "### Interpretation Guide\n",
    "\n",
    "| Effect Size (d) | Interpretation | Typical Impact |\n",
    "|-----------------|----------------|----------------|\n",
    "| < 0.2 | Negligible | Hard to notice in practice |\n",
    "| 0.2 - 0.5 | Small | Noticeable with careful observation |\n",
    "| 0.5 - 0.8 | Medium | Clearly noticeable |\n",
    "| > 0.8 | Large | Obvious, substantial impact |\n",
    "\n",
    "### Statistical Significance vs Practical Significance\n",
    "\n",
    "- **p < 0.05**: Statistically significant (unlikely due to chance)\n",
    "- **Cohen's d**: Practical significance (magnitude of difference)\n",
    "- Both should be considered when interpreting results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
