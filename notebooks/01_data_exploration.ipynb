{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDOS Data Exploration\n",
    "\n",
    "This notebook provides interactive exploration of experiment results from the LDOS tracing harness.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Data Loading\n",
    "2. Data Overview\n",
    "3. Success Rate Analysis\n",
    "4. Latency Distribution Visualization\n",
    "5. Outlier Detection\n",
    "6. Scenario Comparison\n",
    "7. Export Filtered Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Project paths\n",
    "WS_ROOT = Path('..').resolve()\n",
    "RESULTS_DIR = WS_ROOT / 'results'\n",
    "ANALYSIS_OUTPUT = WS_ROOT / 'analysis' / 'output'\n",
    "\n",
    "print(f\"Workspace: {WS_ROOT}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Available result directories: {[d.name for d in RESULTS_DIR.iterdir() if d.is_dir()] if RESULTS_DIR.exists() else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_combined_results(results_dir: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load all experiment results from the results directory.\n",
    "    Combines results from multiple scenarios into a single DataFrame.\n",
    "    \"\"\"\n",
    "    all_rows = []\n",
    "    \n",
    "    if not results_dir.exists():\n",
    "        print(f\"Results directory not found: {results_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    for scenario_dir in results_dir.iterdir():\n",
    "        if not scenario_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        scenario_name = scenario_dir.name\n",
    "        \n",
    "        # Load JSON result files\n",
    "        json_files = list(scenario_dir.glob('*_result.json'))\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            try:\n",
    "                with open(json_file) as f:\n",
    "                    data = json.load(f)\n",
    "                data['scenario'] = scenario_name\n",
    "                data['source_file'] = json_file.name\n",
    "                all_rows.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load {json_file}: {e}\")\n",
    "        \n",
    "        # Also try loading summary CSV if exists\n",
    "        summary_csv = scenario_dir / 'summary.csv'\n",
    "        if summary_csv.exists():\n",
    "            try:\n",
    "                df_summary = pd.read_csv(summary_csv)\n",
    "                df_summary['scenario'] = scenario_name\n",
    "                df_summary['source_file'] = 'summary.csv'\n",
    "                # Append rows that aren't already loaded\n",
    "                if len(json_files) == 0:\n",
    "                    all_rows.extend(df_summary.to_dict('records'))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load {summary_csv}: {e}\")\n",
    "    \n",
    "    if not all_rows:\n",
    "        print(\"No data found. Run experiments first with: make run_all\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(all_rows)\n",
    "    print(f\"Loaded {len(df)} trials from {df['scenario'].nunique()} scenarios\")\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_combined_results(RESULTS_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "if len(df) > 0:\n",
    "    print(\"=== DataFrame Info ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {list(df.columns)}\")\n",
    "    print(f\"\\nScenarios: {df['scenario'].unique().tolist()}\")\n",
    "    print(f\"\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "else:\n",
    "    print(\"No data loaded. Please run experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "if len(df) > 0:\n",
    "    # Identify numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    latency_cols = [c for c in numeric_cols if 'latency' in c.lower() or 'ms' in c.lower()]\n",
    "    \n",
    "    print(\"=== Latency Columns ===\")\n",
    "    print(latency_cols)\n",
    "    \n",
    "    if latency_cols:\n",
    "        print(\"\\n=== Latency Statistics ===\")\n",
    "        display(df[latency_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "if len(df) > 0:\n",
    "    print(\"=== Sample Rows ===\")\n",
    "    display(df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Success Rate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0 and 'status' in df.columns:\n",
    "    # Success rate per scenario\n",
    "    success_rates = df.groupby('scenario').apply(\n",
    "        lambda x: (x['status'] == 'success').mean() * 100\n",
    "    ).reset_index(name='success_rate_pct')\n",
    "    \n",
    "    print(\"=== Success Rate by Scenario ===\")\n",
    "    display(success_rates)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    bars = ax.bar(success_rates['scenario'], success_rates['success_rate_pct'], \n",
    "                  color=sns.color_palette('husl', len(success_rates)))\n",
    "    ax.axhline(y=90, color='r', linestyle='--', label='90% threshold')\n",
    "    ax.set_xlabel('Scenario')\n",
    "    ax.set_ylabel('Success Rate (%)')\n",
    "    ax.set_title('Success Rate by Scenario')\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, success_rates['success_rate_pct']):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f'{rate:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "elif len(df) > 0:\n",
    "    print(\"No 'status' column found. All trials assumed successful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trial counts\n",
    "if len(df) > 0:\n",
    "    trial_counts = df.groupby('scenario').size().reset_index(name='n_trials')\n",
    "    print(\"=== Trial Counts ===\")\n",
    "    display(trial_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Latency Distribution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latency_distributions(df: pd.DataFrame, latency_col: str = 'total_latency_ms'):\n",
    "    \"\"\"Plot latency distributions across scenarios.\"\"\"\n",
    "    if latency_col not in df.columns:\n",
    "        print(f\"Column '{latency_col}' not found. Available: {df.columns.tolist()}\")\n",
    "        return\n",
    "    \n",
    "    # Filter successful trials if status exists\n",
    "    if 'status' in df.columns:\n",
    "        df_plot = df[df['status'] == 'success'].copy()\n",
    "    else:\n",
    "        df_plot = df.copy()\n",
    "    \n",
    "    scenarios = df_plot['scenario'].unique()\n",
    "    n_scenarios = len(scenarios)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Histogram per scenario\n",
    "    ax1 = axes[0, 0]\n",
    "    for scenario in scenarios:\n",
    "        data = df_plot[df_plot['scenario'] == scenario][latency_col].dropna()\n",
    "        ax1.hist(data, bins=30, alpha=0.6, label=scenario, density=True)\n",
    "    ax1.set_xlabel(f'{latency_col}')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.set_title('Latency Distribution by Scenario')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Box plot\n",
    "    ax2 = axes[0, 1]\n",
    "    df_plot.boxplot(column=latency_col, by='scenario', ax=ax2)\n",
    "    ax2.set_xlabel('Scenario')\n",
    "    ax2.set_ylabel(f'{latency_col}')\n",
    "    ax2.set_title('Latency Box Plot')\n",
    "    plt.suptitle('')  # Remove auto-title\n",
    "    \n",
    "    # 3. Violin plot\n",
    "    ax3 = axes[1, 0]\n",
    "    sns.violinplot(data=df_plot, x='scenario', y=latency_col, ax=ax3)\n",
    "    ax3.set_xlabel('Scenario')\n",
    "    ax3.set_ylabel(f'{latency_col}')\n",
    "    ax3.set_title('Latency Violin Plot')\n",
    "    \n",
    "    # 4. CDF\n",
    "    ax4 = axes[1, 1]\n",
    "    for scenario in scenarios:\n",
    "        data = df_plot[df_plot['scenario'] == scenario][latency_col].dropna().sort_values()\n",
    "        cdf = np.arange(1, len(data) + 1) / len(data)\n",
    "        ax4.plot(data, cdf, label=scenario, linewidth=2)\n",
    "    ax4.set_xlabel(f'{latency_col}')\n",
    "    ax4.set_ylabel('CDF')\n",
    "    ax4.set_title('Cumulative Distribution Function')\n",
    "    ax4.legend()\n",
    "    ax4.axhline(y=0.95, color='r', linestyle='--', alpha=0.5, label='P95')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if len(df) > 0:\n",
    "    # Try common latency column names\n",
    "    for col in ['total_latency_ms', 'planning_latency_ms', 'execution_latency_ms', 'latency_ms']:\n",
    "        if col in df.columns:\n",
    "            plot_latency_distributions(df, col)\n",
    "            break\n",
    "    else:\n",
    "        print(\"No latency columns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple latency metrics\n",
    "if len(df) > 0:\n",
    "    latency_cols = [c for c in df.columns if 'latency' in c.lower()]\n",
    "    \n",
    "    if len(latency_cols) > 1:\n",
    "        # Filter successful\n",
    "        if 'status' in df.columns:\n",
    "            df_success = df[df['status'] == 'success']\n",
    "        else:\n",
    "            df_success = df\n",
    "        \n",
    "        # Melt for plotting\n",
    "        df_melted = df_success.melt(\n",
    "            id_vars=['scenario'],\n",
    "            value_vars=latency_cols,\n",
    "            var_name='metric',\n",
    "            value_name='latency_ms'\n",
    "        )\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        sns.boxplot(data=df_melted, x='scenario', y='latency_ms', hue='metric', ax=ax)\n",
    "        ax.set_xlabel('Scenario')\n",
    "        ax.set_ylabel('Latency (ms)')\n",
    "        ax.set_title('Latency Breakdown by Scenario')\n",
    "        ax.legend(title='Metric', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_outliers_iqr(df: pd.DataFrame, column: str, k: float = 1.5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Detect outliers using the IQR method.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame\n",
    "        column: Column to check for outliers\n",
    "        k: IQR multiplier (default 1.5 for standard outliers, 3.0 for extreme)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with outlier rows\n",
    "    \"\"\"\n",
    "    if column not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    data = df[column].dropna()\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - k * IQR\n",
    "    upper_bound = Q3 + k * IQR\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    \n",
    "    print(f\"=== Outlier Detection for {column} ===\")\n",
    "    print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"Bounds: [{lower_bound:.2f}, {upper_bound:.2f}]\")\n",
    "    print(f\"Outliers: {len(outliers)} / {len(df)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    return outliers\n",
    "\n",
    "if len(df) > 0:\n",
    "    for col in ['total_latency_ms', 'planning_latency_ms', 'execution_latency_ms']:\n",
    "        if col in df.columns:\n",
    "            outliers = detect_outliers_iqr(df, col)\n",
    "            if len(outliers) > 0:\n",
    "                print(f\"\\nOutlier scenarios: {outliers['scenario'].value_counts().to_dict()}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers\n",
    "if len(df) > 0 and 'total_latency_ms' in df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    col = 'total_latency_ms'\n",
    "    \n",
    "    # Box plot showing outliers\n",
    "    ax1 = axes[0]\n",
    "    df.boxplot(column=col, by='scenario', ax=ax1, showfliers=True)\n",
    "    ax1.set_title('With Outliers')\n",
    "    ax1.set_xlabel('Scenario')\n",
    "    ax1.set_ylabel(col)\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # Without extreme outliers\n",
    "    ax2 = axes[1]\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    df_no_extreme = df[(df[col] >= Q1 - 3*IQR) & (df[col] <= Q3 + 3*IQR)]\n",
    "    df_no_extreme.boxplot(column=col, by='scenario', ax=ax2, showfliers=True)\n",
    "    ax2.set_title('Without Extreme Outliers (3*IQR)')\n",
    "    ax2.set_xlabel('Scenario')\n",
    "    ax2.set_ylabel(col)\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scenario Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scenarios_summary(df: pd.DataFrame, latency_col: str = 'total_latency_ms') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate summary statistics per scenario.\n",
    "    \"\"\"\n",
    "    if latency_col not in df.columns:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Filter successful\n",
    "    if 'status' in df.columns:\n",
    "        df_success = df[df['status'] == 'success']\n",
    "    else:\n",
    "        df_success = df\n",
    "    \n",
    "    summary = df_success.groupby('scenario')[latency_col].agg([\n",
    "        ('n_trials', 'count'),\n",
    "        ('mean', 'mean'),\n",
    "        ('std', 'std'),\n",
    "        ('min', 'min'),\n",
    "        ('p25', lambda x: x.quantile(0.25)),\n",
    "        ('median', 'median'),\n",
    "        ('p75', lambda x: x.quantile(0.75)),\n",
    "        ('p95', lambda x: x.quantile(0.95)),\n",
    "        ('p99', lambda x: x.quantile(0.99)),\n",
    "        ('max', 'max'),\n",
    "    ]).round(2)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "if len(df) > 0:\n",
    "    for col in ['total_latency_ms', 'planning_latency_ms', 'execution_latency_ms']:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n=== {col} Summary ===\")\n",
    "            summary = compare_scenarios_summary(df, col)\n",
    "            display(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage change from baseline\n",
    "if len(df) > 0 and 'scenario' in df.columns and 'total_latency_ms' in df.columns:\n",
    "    col = 'total_latency_ms'\n",
    "    \n",
    "    # Filter successful\n",
    "    if 'status' in df.columns:\n",
    "        df_success = df[df['status'] == 'success']\n",
    "    else:\n",
    "        df_success = df\n",
    "    \n",
    "    means = df_success.groupby('scenario')[col].mean()\n",
    "    \n",
    "    if 'baseline' in means.index:\n",
    "        baseline_mean = means['baseline']\n",
    "        pct_change = ((means - baseline_mean) / baseline_mean * 100).round(1)\n",
    "        \n",
    "        print(\"=== Percentage Change from Baseline ===\")\n",
    "        for scenario, change in pct_change.items():\n",
    "            if scenario != 'baseline':\n",
    "                print(f\"  {scenario}: {change:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_filtered_data(df: pd.DataFrame, output_dir: Path, prefix: str = 'filtered'):\n",
    "    \"\"\"\n",
    "    Export filtered datasets for further analysis.\n",
    "    \"\"\"\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Full dataset\n",
    "    full_path = output_dir / f'{prefix}_all.csv'\n",
    "    df.to_csv(full_path, index=False)\n",
    "    print(f\"Exported all data: {full_path}\")\n",
    "    \n",
    "    # Successful trials only\n",
    "    if 'status' in df.columns:\n",
    "        df_success = df[df['status'] == 'success']\n",
    "        success_path = output_dir / f'{prefix}_success_only.csv'\n",
    "        df_success.to_csv(success_path, index=False)\n",
    "        print(f\"Exported successful trials: {success_path}\")\n",
    "    \n",
    "    # Per-scenario exports\n",
    "    for scenario in df['scenario'].unique():\n",
    "        scenario_df = df[df['scenario'] == scenario]\n",
    "        scenario_path = output_dir / f'{prefix}_{scenario}.csv'\n",
    "        scenario_df.to_csv(scenario_path, index=False)\n",
    "        print(f\"Exported {scenario}: {scenario_path}\")\n",
    "\n",
    "# Uncomment to export\n",
    "# if len(df) > 0:\n",
    "#     export_filtered_data(df, ANALYSIS_OUTPUT / 'filtered_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export summary table for LaTeX\n",
    "def export_latex_table(df: pd.DataFrame, output_path: Path, latency_col: str = 'total_latency_ms'):\n",
    "    \"\"\"\n",
    "    Export summary as LaTeX table.\n",
    "    \"\"\"\n",
    "    summary = compare_scenarios_summary(df, latency_col)\n",
    "    \n",
    "    # Format for LaTeX\n",
    "    latex = summary[['n_trials', 'mean', 'std', 'p95', 'max']].to_latex(\n",
    "        float_format='%.2f',\n",
    "        caption=f'Latency Summary ({latency_col})',\n",
    "        label='tab:latency_summary'\n",
    "    )\n",
    "    \n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.write(latex)\n",
    "    \n",
    "    print(f\"Exported LaTeX table: {output_path}\")\n",
    "    print(\"\\n\" + latex)\n",
    "\n",
    "# Uncomment to export\n",
    "# if len(df) > 0:\n",
    "#     export_latex_table(df, ANALYSIS_OUTPUT / 'tables' / 'latency_summary.tex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "After exploring the data, proceed to:\n",
    "\n",
    "1. **`02_statistical_analysis.ipynb`** - Statistical tests, confidence intervals, effect sizes\n",
    "2. **`03_path_analysis.ipynb`** - End-to-end path analysis and callback chain visualization\n",
    "\n",
    "Run experiments with:\n",
    "```bash\n",
    "cd ~/ldos_manip_tracing\n",
    "make run_all NUM_TRIALS=30\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
